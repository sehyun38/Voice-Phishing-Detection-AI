import os
import pandas as pd
from transformers import AutoModel, AutoTokenizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.cluster import KMeans
import numpy as np
from tqdm import tqdm
import torch

# --- í™˜ê²½ë³€ìˆ˜ ì„¤ì • (Windows MKL ë©”ëª¨ë¦¬ ì´ìŠˆ ì™„í™”ìš©) ---
os.environ["OMP_NUM_THREADS"] = "8"

# --- ì‚¬ìš©ì ì„¤ì • ë³€ìˆ˜ ---
MODEL_NAME = "BM-K/KoSimCSE-roberta"
CLUSTER_COUNT = 3
REMOVE_DUPLICATES = False  # ê°œì„ : ì¤‘ë³µ ì œê±° ëŒ€ì‹  í‘œì‹œë§Œ
VERBOSE = True
INPUT_FILE = "../ë³´ë¥˜/Interactive_VP_Dataset_exaone_360_v1.csv"
OUTPUT_FILE = "../dataset/Interactive_Dataset/vp_clustered_exaone_360_cleaned.csv"
IMPORTANT_KEYWORDS_FILE = "../dataset/phishing_words.csv"
SIMILARITY_THRESHOLD_LOW = 0.8
SIMILARITY_THRESHOLD_HIGH = 0.9
MIN_TOKEN_LENGTH = 10

# --- ë¡œê·¸ ì¶œë ¥ ---
def log(msg, level="INFO", force=False):
    if VERBOSE or force:
        print(f"[{level}] {msg}")

# --- ë°ì´í„° ë¡œë”© ---
def load_data(file_path):
    try:
        df = pd.read_csv(file_path, encoding="utf-8", header=0)
    except:
        df = pd.read_csv(file_path, encoding="cp949", header=0)

    df.columns = df.columns.str.strip()

    log(f"âœ… ì „ì²´ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df)}ê°œ")
    log(f"ğŸ“‚ ì»¬ëŸ¼ ëª©ë¡: {df.columns.tolist()}")
    log(f"ğŸ“Š ì»¬ëŸ¼ë³„ dtype:\n{df.dtypes}")

    return df

# --- ì¤‘ìš” ë‹¨ì–´ ë¡œë“œ ---
def load_important_keywords(path):
    try:
        try:
            df = pd.read_csv(path, encoding="utf-8")
        except:
            df = pd.read_csv(path, encoding="cp949")
        keywords = df['word'].dropna().astype(str).tolist()
        log(f"ì¤‘ìš” í‚¤ì›Œë“œ {len(keywords)}ê°œ ë¡œë“œë¨: {keywords[:10]}...")
        return keywords
    except Exception as e:
        log(f"ì¤‘ìš” ë‹¨ì–´ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}", level="ERROR", force=True)
        return []

# --- ì§§ì€ ë¬¸ì¥ í•„í„°ë§ ---
def filter_short_texts(df, important_keywords, min_token=10):
    def keep_row(row):
        if row["token_count"] >= min_token:
            return True
        for keyword in important_keywords:
            if keyword in row["transcript"]:
                return True
        return False

    before = len(df)
    df_filtered = df[df.apply(keep_row, axis=1)].reset_index(drop=True)
    log(f"ìµœì†Œ {min_token}í† í° í•„í„° ì ìš©: {before} â†’ {len(df_filtered)}ê°œ ìœ ì§€")
    return df_filtered

# --- ë¬¸ì¥ ì„ë² ë”© ---
def get_embeddings(model_name, texts):
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
    model = AutoModel.from_pretrained(model_name, trust_remote_code=True)

    embeddings = []
    skipped = 0
    for idx, text in enumerate(tqdm(texts, desc="ì„ë² ë”© ìƒì„±", unit="ë¬¸ì¥")):
        if not isinstance(text, str) or len(text.strip()) < 3:
            log(f"ê±´ë„ˆëœ€ (too short): idx={idx}, text='{text}'", level="WARN")
            skipped += 1
            continue

        inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True)
        if inputs['input_ids'].shape[1] < 3:
            log(f"ê±´ë„ˆëœ€ (token shortage): idx={idx}, text='{text}'", level="WARN")
            skipped += 1
            continue

        with torch.no_grad():
            outputs = model(**inputs)
            emb = outputs.last_hidden_state.mean(dim=1).squeeze()
            embeddings.append(emb.numpy())

    log(f"ì„ë² ë”© ì™„ë£Œ: {len(embeddings)}ê°œ / ìŠ¤í‚µ: {skipped}ê°œ")
    return embeddings

# --- í´ëŸ¬ìŠ¤í„°ë§ ---
def apply_clustering(embeddings, n_clusters):
    log(f"KMeans í´ëŸ¬ìŠ¤í„°ë§ (ê°œìˆ˜: {n_clusters})")
    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
    kmeans.fit(embeddings)
    log("Clustering done")
    return kmeans.labels_

# --- ì¤‘ë³µ ë¬¸ì¥ ì²˜ë¦¬ ---
def mark_duplicates(df, remove_duplicates=False,
                    similarity_threshold_low=SIMILARITY_THRESHOLD_LOW,
                    similarity_threshold_high=SIMILARITY_THRESHOLD_HIGH):
    embeddings = get_embeddings(MODEL_NAME, df['transcript'].tolist())
    sim_matrix = cosine_similarity(embeddings)

    is_duplicate = np.zeros(len(df), dtype=bool)
    max_similarities = np.zeros(len(df))
    most_similar_indices = np.full(len(df), -1)

    for i in tqdm(range(len(df)), desc="ì¤‘ë³µ ë¬¸ì¥ ë¹„êµ", unit="ë¬¸ì¥"):
        for j in range(len(df)):
            if i == j:
                continue
            sim = sim_matrix[i][j]
            if sim > max_similarities[i]:
                max_similarities[i] = sim
                most_similar_indices[i] = j
            if sim >= similarity_threshold_high:
                is_duplicate[j] = True
                log(f"{round(sim*100, 2)}% ìœ ì‚¬ â†’ ë¬¸ì¥ {i+1}ê³¼ {j+1}", level="DUPLICATE")
            elif sim >= similarity_threshold_low:
                is_duplicate[j] = True
                log(f"{round(sim*100, 2)}% ìœ ì‚¬ â†’ ë¬¸ì¥ {i+1}ê³¼ {j+1}", level="SIMILAR")

    df['is_duplicate'] = is_duplicate
    df['most_similar_id'] = df.iloc[most_similar_indices.astype(int)]['id'].values
    df['max_similarity'] = np.round(max_similarities * 100, 2)

    return df

# --- í´ëŸ¬ìŠ¤í„°ë§ ì „ì²´ ì‹¤í–‰ ---
def cluster_and_save(df, model_name, n_clusters, output_file, remove_duplicates):
    df = df.copy()
    df = df.reset_index(drop=False).rename(columns={"index": "original_index"})

    log(f"í´ëŸ¬ìŠ¤í„°ë§ ëŒ€ìƒ: {len(df)}")
    embeddings = get_embeddings(model_name, df['transcript'].tolist())
    cluster_labels = apply_clustering(embeddings, n_clusters)
    df['cluster'] = cluster_labels + 1
    df = mark_duplicates(df, remove_duplicates)

    if output_file:
        df.to_csv(output_file, index=False, encoding="utf-8")
        log(f"ê²°ê³¼ ì €ì¥ â†’ {output_file}", force=True)

    return df[["original_index", "cluster", "is_duplicate", "most_similar_id", "max_similarity"]]

# --- ë©”ì¸ ì‹¤í–‰ ---
def main():
    df = load_data(INPUT_FILE)
    df["label"] = pd.to_numeric(df["label"], errors="coerce")
    df["token_count"] = pd.to_numeric(df["token_count"], errors="coerce")

    important_keywords = load_important_keywords(IMPORTANT_KEYWORDS_FILE)

    phishing_df = df[df["label"] == 1].reset_index(drop=True)
    log(f"label==1 ë°ì´í„°: {len(phishing_df)}")

    phishing_df_filtered = filter_short_texts(phishing_df, important_keywords, min_token=MIN_TOKEN_LENGTH)
    clustered_df = cluster_and_save(phishing_df_filtered, MODEL_NAME, CLUSTER_COUNT, None, REMOVE_DUPLICATES)

    # ì›ë³¸ì— ë³€ê²½ ì ìš©
    df["cluster"] = None
    df["is_duplicate"] = False
    df["most_similar_id"] = None
    df["max_similarity"] = None

    for _, row in clustered_df.iterrows():
        idx = row["original_index"]
        df.at[idx, "cluster"] = row["cluster"]
        df.at[idx, "is_duplicate"] = row["is_duplicate"]
        df.at[idx, "most_similar_id"] = row["most_similar_id"]
        df.at[idx, "max_similarity"] = row["max_similarity"]

    df.to_csv(OUTPUT_FILE, index=False, encoding="utf-8")
    log(f"ìµœì¢… ê²°ê³¼ ì €ì¥ ì™„ë£Œ â†’ {OUTPUT_FILE}", force=True)

    # --- í´ëŸ¬ìŠ¤í„°ë³„ ê°œìˆ˜ ì¶œë ¥ ---
    if "cluster" in df.columns:
        log("í´ëŸ¬ìŠ¤í„°ë³„ ë¬¸ì¥ ìˆ˜:", force=True)
        cluster_counts = df[df["label"] == 1]["cluster"].value_counts().sort_index()
        for cluster_id, count in cluster_counts.items():
            log(f"  â–¶ Cluster {int(cluster_id)}: {count}ê°œ", force=True)

        # ë¯¸ë¶„ë¥˜ëœ label==1 ë¬¸ì¥ í™•ì¸
        unclustered = df[(df["label"] == 1) & (df["cluster"].isna())]
        log(f"â— í´ëŸ¬ìŠ¤í„°ì— í¬í•¨ë˜ì§€ ì•Šì€ label==1 ë¬¸ì¥ ìˆ˜: {len(unclustered)}ê°œ", force=True)

if __name__ == "__main__":
    main()
